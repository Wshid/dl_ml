## CH00_01. 강의 소개 - 딥러닝이 나오기까지

### 강의 소개
- 딥러닝을 위한 머신 러닝 기초 학습
- feed-forward neural netowrk
- 딥러닝 학습의 Regularization 방법
- 딥러닝 학습의 최적화(optimization) 방법
- Convolution neural network
- Recurrent neural network; RNN, Attention
- Transformer & 자가-주의 집중망(self-attention)
- Large-scale의 Pretrained Transformer(GPT, BERT)
- Deep Generative 모델들
  - Auto-regressive / Sequence-to-sequence / Auto-encoder / Embedding
  - Flow & IAF 모델
  - LAtent Variable Model(Variational Inference; VAE)
  - Implicit Models(GAN)
- Metric LEarning
- Reinforcement Learning & Deep RL
- Meta-learning
- Production을 위한 기본적인 Research Engineering 기법 소개

### Pytorch, Tensorflow vs Jax
- Pytorch, Tensorflow를 많이 씀
- Jax는 Lab에서 많이 씀
- Tensorflow에서도 V2부터 쓰기 쉬워짐

### 기술 스택의 특징
- 연구에서는 2020년 이후로 PyTorch > Tensorflow
- 가장 최신 연구는 높은 확률로 Tensorflow < Pytorch
- Tensorflow V2가 많이 편해졌으나, 아직 문서는 V1과 혼재
- Pytorch가 Research Engineering에서는 조금 부족하나, 많이 따라옴
- 최근엔 MLOps와 함께 model exchange 및 모델 서빙 방법이 많이 연구됨에 따라
  - framework-agnostic AI model serving이 가능해지고
  - 어떤 프레임워크를 쓰든 크게 상관이 없어짐
- 현재는 Tensorflow V2와 Pytorch 둘 중 편한것을 골라 사용하면 됨

### 강의내 기술 스택
- PyTorch, Tensorflow, PyTorch Lightning
- TensorBoard, Weights & Biases
- HYDRA, ONNX
